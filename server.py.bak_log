"""
Obsidian Semantic Search MCP Server

FastMCP server providing semantic search capabilities for Obsidian vaults.
Exposes tools for searching notes, indexing, and managing the vector store.
"""

import os
from pathlib import Path
from typing import Optional, Dict, Any, List
from fastmcp import FastMCP
from config import Config
from embeddings import EmbeddingService
from vector_store import VectorStore
from indexer import VaultIndexer
from chunker import MarkdownChunker
from contextlib import asynccontextmanager

# Global instances (lazy-initialized)
_config: Optional[Config] = None
_embedding_service: Optional[EmbeddingService] = None
_vector_store: Optional[VectorStore] = None
_indexer: Optional[VaultIndexer] = None

def get_config() -> Config:
    """Get or initialize configuration."""
    global _config
    if _config is None:
        _config = Config()
    return _config

def get_embedding_service() -> EmbeddingService:
    """Get or initialize embedding service."""
    global _embedding_service
    if _embedding_service is None:
        config = get_config()
        _embedding_service = EmbeddingService(
            api_key=config.openai_api_key,
            model=config.embedding_model,
            batch_size=config.embedding_batch_size
        )
    return _embedding_service

def get_vector_store() -> VectorStore:
    """Get or initialize vector store."""
    global _vector_store
    if _vector_store is None:
        config = get_config()
        _vector_store = VectorStore(
            persist_directory=str(config.chromadb_path)
        )
    return _vector_store

def get_indexer() -> VaultIndexer:
    """Get or initialize vault indexer."""
    global _indexer
    if _indexer is None:
        config = get_config()
        vector_store = get_vector_store()
        embedding_service = get_embedding_service()
        chunker = MarkdownChunker(
            target_chunk_size=config.target_chunk_size,
            max_chunk_size=config.max_chunk_size,
            min_chunk_size=config.min_chunk_size
        )
        _indexer = VaultIndexer(
            vault_path=config.vault_path,
            vector_store=vector_store,
            embedding_service=embedding_service,
            chunker=chunker
        )
    return _indexer

@asynccontextmanager
async def lifespan(server: FastMCP):
    """Manage server lifecycle (startup/shutdown)."""
    import sys
    watcher = None
    try:
        # Startup logic (previously _prewarm)
        print("Pre-warming services...", file=sys.stderr)
        get_config()
        get_embedding_service()
        get_vector_store()
        get_indexer()
        print("Services pre-warmed successfully.", file=sys.stderr)

        # Start Watcher if enabled
        config = get_config()
        if os.environ.get("WATCH_MODE", "false").lower() == "true":
            from watcher import VaultWatcher
            
            print("Auto-indexing enabled (WATCH_MODE=true)", file=sys.stderr)
            watcher = VaultWatcher(
                vault_path=config.vault_path,
                indexer=get_indexer(),
                vector_store=get_vector_store()
            )
            watcher.start()
            
        yield
        
    finally:
        # Shutdown logic
        if watcher:
            print("Stopping Vault Watcher...", file=sys.stderr)
            watcher.stop()
            print("Vault Watcher stopped.", file=sys.stderr)

# Initialize FastMCP server with lifespan
mcp = FastMCP("obsidian-semantic-search", lifespan=lifespan)


@mcp.tool()
def semantic_search(
    query: str,
    n_results: int = 5,
    folder: Optional[str] = None,
    tags: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Search vault semantically for notes matching the query.

    Args:
        query: Natural language search query
        n_results: Number of results to return (default: 5)
        folder: Optional folder filter (e.g., "1-projects/trading")
        tags: Optional comma-separated tags filter (e.g., "trading,gold")

    Returns:
        List of search results with content, metadata, and similarity scores
    """
    try:
        # Get services
        embedding_service = get_embedding_service()
        vector_store = get_vector_store()
        config = get_config()

        # Generate query embedding
        query_embedding = embedding_service.embed_single(query)

        # Build metadata filter
        where_filter = None
        if folder or tags:
            where_filter = {}
            if folder:
                where_filter["folder"] = folder
            if tags:
                # Note: ChromaDB metadata stores tags as comma-separated string
                # For now, we'll filter by exact tag string match
                # In production, you might want more flexible tag matching
                where_filter["tags"] = tags

        # Query vector store
        results = vector_store.query(
            query_embedding=query_embedding,
            n_results=n_results,
            where=where_filter
        )

        # Format results with similarity scores
        formatted_results = []
        for i in range(len(results['ids'])):
            distance = results['distances'][i]
            similarity = 1 - distance  # Convert distance to similarity

            metadata = results['metadatas'][i]

            # Convert comma-separated tags back to list
            tags_str = metadata.get('tags', '')
            tags_list = [t.strip() for t in tags_str.split(',') if t.strip()]

            result = {
                'id': results['ids'][i],
                'content': results['documents'][i],
                'similarity': round(similarity, 3),
                'file_path': metadata.get('file_path', ''),
                'note_title': metadata.get('note_title', ''),
                'header_context': metadata.get('header_context', ''),
                'folder': metadata.get('folder', ''),
                'tags': tags_list,
                'chunk_index': metadata.get('chunk_index', 0),
                'token_count': metadata.get('token_count', 0)
            }
            formatted_results.append(result)

        return formatted_results

    except Exception as e:
        return [{
            'error': f"Search failed: {str(e)}",
            'query': query
        }]


@mcp.tool()
def reindex_vault(force: bool = False) -> Dict[str, Any]:
    """
    Rebuild or update the entire vault index.

    Args:
        force: If True, reindex all files regardless of changes.
               If False, only index new/modified files (incremental).

    Returns:
        Dictionary with indexing statistics:
        - notes_processed: Number of notes indexed
        - notes_skipped: Number of unchanged notes skipped
        - chunks_created: Total chunks generated
        - duration_seconds: Time taken
        - errors: List of any errors encountered
    """
    try:
        indexer = get_indexer()
        result = indexer.index_vault(force=force)

        return {
            'success': True,
            'notes_processed': result.notes_processed,
            'notes_skipped': result.notes_skipped,
            'chunks_created': result.chunks_created,
            'duration_seconds': round(result.duration_seconds, 2),
            'errors': result.errors
        }

    except Exception as e:
        return {
            'success': False,
            'error': f"Indexing failed: {str(e)}",
            'notes_processed': 0,
            'notes_skipped': 0,
            'chunks_created': 0,
            'duration_seconds': 0,
            'errors': [str(e)]
        }


@mcp.tool()
def get_index_stats() -> Dict[str, Any]:
    """
    Get statistics about the current index.

    Returns:
        Dictionary with index statistics:
        - total_chunks: Number of chunks in index
        - total_files: Number of files indexed
        - vault_path: Path to vault
        - chromadb_path: Path to ChromaDB storage
        - embedding_model: Embedding model used
    """
    try:
        config = get_config()
        vector_store = get_vector_store()

        stats = vector_store.get_stats()

        return {
            'total_chunks': stats['total_chunks'],
            'total_files': stats['total_files'],
            'vault_path': str(config.vault_path),
            'chromadb_path': stats['persist_directory'],
            'embedding_model': config.embedding_model,
            'collection_name': stats['collection_name']
        }


    except Exception as e:
        return {
            'error': f"Failed to get stats: {str(e)}",
            'total_chunks': 0,
            'total_files': 0
        }





@mcp.tool()
def suggest_links(
    note_path: str,
    n_suggestions: int = 5,
    min_similarity: float = 0.5,
    exclude_current: bool = True,
    folder: Optional[str] = None,
    tags: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Suggest related notes to link based on content similarity.
    OPTIMIZED: reuses existing embeddings if file hasn't changed.

    Args:
        note_path: Path to note (e.g., "1-projects/trading.md")
        n_suggestions: Number of suggestions
        min_similarity: Minimum similarity threshold
        exclude_current: Exclude chunks from same note
        folder: Optional folder filter for suggestions
        tags: Optional comma-separated tags filter for suggestions
    """
    
    from collections import defaultdict
    import statistics
    from utils import compute_content_hash

    try:
        config = get_config()
        vector_store = get_vector_store()
        embedding_service = get_embedding_service()
        indexer = get_indexer()

        # 1. Read target note
        abs_path = config.vault_path / note_path
        if not abs_path.exists():
            return [{'error': f"File not found: {note_path}"}]

        with open(abs_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # 2. Check for cached embeddings (Smart Caching)
        current_hash = str(compute_content_hash(content)).strip()
        stored_hash = vector_store.check_content_hash(note_path)
        if stored_hash:
            stored_hash = str(stored_hash).strip()
        
        embeddings = []
        
        if stored_hash and stored_hash == current_hash:
            # Case A: File unchanged, reuse embeddings!
            # print(f"DEBUG: Cache hit for {note_path}", file=sys.stderr)
            file_data = vector_store.get_by_file_path(note_path)
            embeddings = file_data.get('embeddings', [])
        else:
            # Case B: File modified or new, must generate
            # print(f"DEBUG: Cache miss for {note_path} (generating)", file=sys.stderr)
            chunks = indexer.chunker.chunk_markdown(content)
             # Extract text from chunks
            chunk_texts = [c.content for c in chunks]
            if chunk_texts:
                embeddings = embedding_service.embed_texts(chunk_texts)

        # Ensure embeddings is a list of lists, not numpy array
        if hasattr(embeddings, 'tolist'):
            embeddings = embeddings.tolist()

        if not embeddings or len(embeddings) == 0:
            return [{'error': "No content to analyze in this note"}]

        # 3. Query for similar chunks (using all embeddings)
        # We query for each chunk and aggregate results
        all_results = []
        
        # Build metadata filter
        where_filter = None
        if folder or tags:
            where_filter = {}
            if folder:
                where_filter["folder"] = folder
            if tags:
                where_filter["tags"] = tags

        for idx, embedding in enumerate(embeddings):
            if hasattr(embedding, 'tolist'):
                embedding = embedding.tolist()

            results = vector_store.query(
                query_embedding=embedding,
                n_results=n_suggestions * 2, # Fetch more to filter
                where=where_filter
            )
            
            # Unpack results
            ids = results['ids']
            distances = results['distances']
            metadatas = results['metadatas']
            
            for i in range(len(ids)):
                match_file = str(metadatas[i].get('file_path', ''))
                
                # Exclude current file
                if exclude_current and match_file == note_path:
                    continue
                
                distance = distances[i]
                # Handle numpy types if present
                if hasattr(distance, 'item'):
                    distance = distance.item()
                
                similarity = 1.0 - float(distance)
                
                if float(similarity) < min_similarity:
                    continue

                all_results.append({
                    'file_path': match_file,
                    'metadata': metadatas[i],
                    'similarity': float(similarity)
                })

        # 4. Aggregate by file
        file_scores = defaultdict(list)
        file_metadata = {}

        for res in all_results:
            fpath = res['file_path']
            file_scores[fpath].append(res['similarity'])
            # Keep metadata of best match
            if fpath not in file_metadata or res['similarity'] > file_metadata[fpath]['similarity']:
                 file_metadata[fpath] = {
                     'metadata': res['metadata'],
                     'similarity': res['similarity']
                 }

        # 5. Rank suggestions
        suggestions = []
        for fpath, scores in file_scores.items():
            avg_score = statistics.mean(scores)
            max_score = max(scores)
            # Weighted score: heavily favor max similarity (relevance) but boost by frequency (coverage)
            final_score = (max_score * 0.7) + (avg_score * 0.3)
            
            meta = file_metadata[fpath]['metadata']
            
            # Format suggested link
            target_header = meta.get('header_context', '')
            # Clean header # syntax
            clean_header = target_header.split(' / ')[-1].replace('#', '').strip() if target_header else ''
            
            link = f"[[{meta.get('note_title')}]]"
            if clean_header:
               link = f"[[{meta.get('note_title')}#{clean_header}]]"

            suggestions.append({
                'file_path': fpath,
                'note_title': meta.get('note_title'),
                'similarity': round(final_score, 3),
                'reason': f"Related to section: {target_header}",
                'suggested_link': link
            })

        # Sort by score descending
        suggestions.sort(key=lambda x: x['similarity'], reverse=True)
        
        return suggestions[:n_suggestions]

    except Exception as e:
        return [{
            'error': f"Suggestion failed: {str(e)}",
            'path': note_path
        }]


@mcp.tool()
def index_note(note_path: str) -> Dict[str, Any]:
    """
    Index (or re-index) a specific note.

    Args:
        note_path: Relative path to note (e.g., "1-projects/new-note.md")
    """
    try:
        config = get_config()
        indexer = get_indexer()

        abs_path = config.vault_path / note_path
        if not abs_path.exists():
            return {'error': f"File not found: {note_path}"}

        # Index the file using the exposed method
        chunks = indexer.index_single_file(abs_path)
        
        return {
            'success': True,
            'file': note_path,
            'chunks_indexed': chunks
        }
    except Exception as e:
        return {
            'success': False,
            'error': f"Indexing failed: {str(e)}",
            'file': note_path
        }


if __name__ == "__main__":
    import sys
    import os # Added import for os

    # Check for SSE mode (for persistent service)
    if "--sse" in sys.argv or os.environ.get("MCP_TRANSPORT") == "sse":
        print("Starting SSE server on port 8765...", file=sys.stderr)
        mcp.run(transport="sse", host="0.0.0.0", port=8765)
    else:
        # Default stdio mode
        mcp.run()
